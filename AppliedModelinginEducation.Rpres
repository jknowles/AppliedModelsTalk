<style>

body, p, td, li, div {
  font-size: 18pt;
  color: white;
}

h1,h2,h3,h4,h5,h6 {
        text-shadow: 0 0 0 #000 !important;
  color: white;
}

.section .reveal .state-background {
   background: black;
}

span.centerImage {
     text-align: center;
}
.section .reveal h2,
.section .reveal h3,
.section .reveal p {
   color: white;
   margin-top: 50px;
}


.reveal blockquote {
  background: black;
  border-left: 10px solid #ccc;
  margin: 1.5em 10px;
  padding: 0.5em 10px;
  quotes: "\201C""\201D""\2018""\2019";
}

.reveal section del {
  color: red;
}

blockquote:before {
  color: #ccc;
  content: open-quote;
  font-size: 4em;
  line-height: 0.1em;
  margin-right: 0.25em;
  vertical-align: -0.4em;
}

blockquote p {
  display inline;
}

.reveal pre {   
  margin-top: 0;
  max-width: 95%;
  border: 1px solid #ccc;
  white-space: pre-wrap;
  margin-bottom: 1em; 
  color: black;
}

.reveal a:not(.image) {
  color: red;
  text-decoration: none;
  -webkit-transition: color .15s ease;
  -moz-transition: color .15s ease;
  -ms-transition: color .15s ease;
  -o-transition: color .15s ease;
  transition: color .15s ease; }

.reveal a:not(.image):hover {
  color: #0000f1;
  text-shadow: none;
  border: none; }

.reveal .roll span:after {
  color: #fff;
  background: #00003f; }

.reveal pre code {
  display: block; padding: 0.5em;
  font-size: 1.6em;
  line-height: 1.1em;
  background-color: white;
  overflow: visible;
  max-height: none;
  word-wrap: normal;
  color: black;
}
.reveal .state-background {
  background: black;
} 

.reveal section p {
  color: white;
}

.reveal section h1 {
  color: white;
}

.reveal section h2 {
  color: white;
}

.reveal section h3 {
  color: white;
}

</style>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": { scale: 100}
  });
</script>

Statistical Learning and Applied Modeling in Education
========================================================
Examples and Concerns
------------------------------------------------------

## **Jared Knowles**
## **01-31-2014**

Motivation
===================================================
incremental: true

- Of the two modeling cultures, we've only learned one
- Computation increases are changing everything
- Data is growing and many problems have different issues
- Prediction is underused and undervalued, and this undermines inference

Some Trends
======================================================

- Available data in education is growing astronomically. 
- Schools are increasingly being asked to provide more services. 
- Policy makers increasingly asked to justify their policies with projections. 
- Timelines are speeding up!
- People are talking about things like "data science" and "big data" (even NSF)
- Academics have to explain their work to a growing list of stakeholders. 

Computers
=====================================================

- Most of your statistics books were written by people who used this:

<img style="height:auto; width:auto; max-width:400px; max-height:300px; display: block; margin:0 auto;" src="img/mainframes.jpg" title="Mainframe" alt="FP"/>

- We will be working in an era unlike all previous generations of statisticians -- 
computers are going to continually get better and better at solving problems

Unparalleled Increase in Computation Leads to Different Solutions
======================================================================


<img src="img/float-point-perf.png" title="Single Thread FP" alt="FP" style="display: block; margin:0 auto;" />

<small>Now we can bootstrap, use MCMC for Bayesian methods, and do multi-model inference</small>

Outline
=====================================================

- The Two Modeling Cultures
- Preparing Data
- Assessing Model Fit
- Applying Models Beyond the Journal Article


What is a model?
===============================

<img src="img/Watson-Crick-DNA-model.jpg" title="Watson and Crick" alt="DNA" style="display: block; margin: auto;" />

What is a statistical model?
===============================

- "All models are wrong, some models are useful" ~ George Box
- Statistical models are mathematical summaries of correlations and probabilities 
of known data
- Being wrong is a **feature of a statistical model**, the goal is to explain 
as much data as possible with as few variables as possible
- The most common in the social sciences is the linear regression model
- Sometimes the goal is **inference** and other times it is **prediction**
- In statistical learning we can think of **supervised** and **unsupervised** cases

Statistical Modeling
=======================================================

It is useful to remember that in statistical modeling, in the **supervised** case, we are looking at the following relationship:

$$ \hat{Y} = \hat{f}(X) $$

In this case $\hat{f}$ represents our estimate of the function that links $X$ and 
$Y$. In traditional linear modeling, $\hat{f}$ takes the form:

$$ \hat{Y} = \alpha + \beta(X) + \epsilon $$

However, there exist limitless alternative $\hat{f}$ which we can explore. Applied modeling techniques help us expand the $\hat{f}$ space we search within.


The Data Modeling Culture
=================================

- Starts philosophically with the idea that we have written down a set of X that 
describe Y with a known functional form that we are testing
- Black box between x and y can be known because the data generating process 
DGP is some functional combination of predictors, parameters, and noise
- Model fit is based on goodness of fit and residual tests

<img src="img/DataModel.png" title="Data Models" alt="FP" style="display: block; margin:0 auto;" />

The Algorithmic Modeling Culture
========================================

- Black box is unknowable - we are not modeling nature but seeking to use similar 
inputs to predict the outputs of the natural process
- Model fit measured by prediction accuracy

<img src="img/AlgoModel.png" title="Algorithmic Models" alt="FP" style="display: block; margin:0 auto;" />

Tukey on Models
====================

> Since no model is to be believed in, no optimization for a single model can offer more than distant guidance. What is needed, and is never more than approximately at hand, is guidance about what to do in a sequence of ever more realistic situations. The analyst of data is lucky if he [or she] has some insight into a few terms of this sequence, particularly those not yet mathematized. ~ John W. Tukey

Gelman on Being a Modeling Pluralist
==========================================

> Schools of statistical thoughts are sometimes jokingly likened to religions. This analogy is not perfect—unlike religions, statistical methods have no supernatural content and make essentially no demands on our personal lives. Looking at the comparison from the other direction, it is possible to be agnostic, atheistic, or simply live one’s life without religion, but it is not really possible to do statistics without some philosophy. ~ Andrew Gelman



Functional forms
==============================

```{r echo=FALSE, fig.width=12, fig.height=8, fig.align='center', fig.cap="Figure Adapted from James et al. 2013"}
library(eeptools)
x    <- c(1, 2, 3, 4, 5, 6, 7, 8)
y    <- c(14, 12, 10, 8, 6, 4, 2, 0)

jitter <- function(x) x + runif(1, min=-.5, max=.5)
x <- sapply(x, jitter)
y <- sapply(y, jitter)

labs <- c("Lasso", "Subset Selection", "Least Squares", 
          "Generalized Additive Models", "KNN", "Trees", "Bagging, Boosting", 
          "Support Vector Machines")

qplot(x, y, geom='text', label=labs) + theme_classic() + 
  scale_x_continuous("Flexibility", limits=c(min(x) - 0.5, max(x) + 0.5)) +
  scale_y_continuous("Interpretability", limits=c(min(y) - 0.5, max(y) + 0.5)) +
  labs(title="Functional Forms and Tradeoffs") + 
  theme(axis.text=element_blank(), axis.ticks=element_blank())
```

<small>Figure adapted from James et al. 2013 (figure 2.7)</small>

Statistical Learning or Statistical Inference?
=================================================================

The line between statistical learning and statistical inference has always been 
blurry and unclear. A few questions can help:
<small>
- Am I interested in accurately estimating unobserved observations based on what 
I have learned in my sample?
- Am I interested in the relationships among the parameters in my sample because 
of a theory I am testing, or because of how they can explain an outcome I am 
interested in?
- Is the data I am using common and relatively untransformed? Will new data be 
created regularly that I can fit the same model to and update?
</small>

Predicting Dropout
=====================================================

Applied Model:
<small>
- Data: Common, transactional, regularly collected at specific timepoints
- Many cohorts with common data
- Interested in learning which students today are likely to dropout in the future
- Want: Confident predictions on likely graduation of new students, used to decide how
to allocate resources and services to students
</small>

***

Inferential Model:
<small>
- Data: national survey data, unlikely to be collected on future observations 
- One cohort is followed in the data set
- Interested in learning if social and emotional concerns are more important than 
academic success in predicting graduation
- Want: unbiased and precise estimates of parameters and if possible ability to make 
causal claims
</small>

Why the Difference?
========================================================

Applied Models:

- Provide information to users about what to expect given certain data
- Serve many goals including prediction of non-observed 
outcomes, summarizing large datasets, measuring uncertainty
- Goals for the model are defined by explicit tradeoffs

***

Inferential Models: 

- Focused on understanding patterns in the current data
- Seek to understand how current data extrapolates to a population
- Estimates population parameters from sample data about relationships between 
inputs and outputs


Some Vocabulary
========================================================

- Training data
- Test data
- Bias (error)
- Variance (error)


***

- Data the model is fit to
- Data the model is applied to, but not fit to, to evaluate model fit
- Refers to the amount of error due to simplifying a complex process
- The amount the $f$ would change if fit to a different training set of data


The Challenge
=================================

- When using a statistical model to make predictions we have to think clearly 
about the data we use to build the model, and the data we will be making 
predictions about
- We may build a model with high **internal validity** for the data at hand, 
but that data may not be representative of the data the model will apply to
- We call this the **training error** and the **test error**
- In inferential statistics we often seek to reduce **training error** and not 
concern ourselves with **test error**
- In applied modeling we focus on finding the optimal tradeoff between **variance** and **bias** 
in order to hopefully reduce **test error**

A Simple Motivating Example
=================================

```{r, echo=FALSE, results='hide', fig.align='center', fig.width=9}
library('quantmod')
library('lubridate')
library('eeptools')

getSymbols("AAPL")
AAPL$year <- year(index(AAPL))
AAPL$yearL <- lag(AAPL$year)
AAPL$yearSTART <- 0
AAPL$yearSTART[AAPL$year - AAPL$yearL > 0] <- 1
AAPL$yearL <- NULL
AAPL$lag <- lag(AAPL$AAPL.Adjusted)
AAPL$lag2 <- lag(AAPL$AAPL.Adjusted, 2)
AAPL$lag3 <- lag(AAPL$AAPL.Adjusted, 3)
AAPL$lag30 <- lag(AAPL$AAPL.Adjusted, 30)
AAPL$lag90 <- lag(AAPL$AAPL.Adjusted, 90)

plotdf <- as.data.frame(AAPL)
plotdf$time <- row.names(plotdf)
plotdf <- plotdf[300:1700,]
plotdf$group <- NA
plotdf$group[50:300] <- 1
plotdf$group[300:500] <- 2

plotdf$time2 <- as.factor(plotdf$time)
plotdf$time2 <- as.numeric(plotdf$time2)

pred.con <- loess.control(surface="direct")

model1 <- loess(AAPL.Adjusted ~ time2 + lag + lag30, 
                data=plotdf[plotdf$group==1,], 
                control=pred.con, parametric=c("lag"))

xrange <- range(plotdf$time2)
xseq <- seq(from=xrange[1], to=xrange[2], length=nrow(plotdf))
pred <- predict(model1, newdata = data.frame(time2 = xseq, lag=plotdf$lag, 
                                             lag30 = plotdf$lag30,
                                             lag90 = plotdf$lag90), se=TRUE)
y = pred$fit
ci <- pred$se.fit * qt(0.95 / 2 + .5, pred$df)
ymin = y - ci
ymax = y + ci
loess.DF <- data.frame(x = xseq, y) #, ymin, ymax, se = pred$se.fit)

model2 <- loess(AAPL.Adjusted ~ time2 + lag + lag30, 
                data=plotdf[500:800,], 
                control=pred.con,  parametric=c("lag"))

xrange <- range(plotdf$time2)
xseq <- seq(from=xrange[1], to=xrange[2], length=nrow(plotdf))
pred <- predict(model2, newdata = data.frame(time2 = xseq, lag=plotdf$lag, 
                                             lag30 = plotdf$lag30,
                                             lag90 = plotdf$lag90), se=TRUE)
y = pred$fit
ci <- pred$se.fit * qt(0.95 / 2 + .5, pred$df)
ymin = y - ci
ymax = y + ci
loess.DF2 <- data.frame(x = xseq, y)


idx1 <- plotdf$time2[plotdf$yearSTART==1][5]
idx2 <- plotdf$time2[plotdf$yearSTART==1][4]
idx3 <- plotdf$time2[plotdf$yearSTART==1][3]
idx4 <- plotdf$time2[plotdf$yearSTART==1][2]
idx5 <- plotdf$time2[plotdf$yearSTART==1][1]

qplot(time, AAPL.Adjusted, data = plotdf, geom='line', group=1) +
     coord_cartesian(xlim=c(0,1400), ylim=c(60,700)) +
  geom_vline(xintercept=idx1, color="red", linetype=3) + 
  geom_vline(xintercept=idx2, color="red", linetype=3) + 
  geom_vline(xintercept=idx3, color="red", linetype=3) + 
  geom_vline(xintercept=idx4, color="red", linetype=3) + 
  geom_vline(xintercept=idx5, color="red", linetype=3) + 
  theme_dpi()+
  theme(axis.text.x=element_blank(), axis.ticks=element_blank(), 
        panel.grid.major.x = element_blank(), 
        panel.grid.minor.x = element_blank()) + 
  labs(x="Time", y="Adjusted Value", title="Apple Stock Values from 2008-2013")

```


Forecasting Apple Stock Could be Useful
===========================================

- Fit a model on the earlier part of the data (in blue)

```{r, echo=FALSE, results='hide', fig.align='center', fig.width=9}

qplot(time2, AAPL.Adjusted, data = plotdf, geom='line', group=1) +
  geom_smooth(aes_auto(loess.DF),  data=loess.DF, stat="identity", se=FALSE) +
     coord_cartesian(xlim=c(0,800), ylim=c(60,700)) +
  geom_vline(xintercept=idx1, color="red", linetype=3) + 
  geom_vline(xintercept=idx2, color="red", linetype=3) + 
  geom_vline(xintercept=idx3, color="red", linetype=3) + 
  geom_vline(xintercept=idx4, color="red", linetype=3) + 
  geom_vline(xintercept=idx5, color="red", linetype=3) + 
  theme_dpi()+
      geom_smooth(method="lm", linetype=2, color=I("red")) +
  theme(axis.text.x=element_blank(), axis.ticks=element_blank(), 
        panel.grid.major.x = element_blank(), 
        panel.grid.minor.x = element_blank()) +
  annotate("rect", xmin=50, xmax=300, ymin=-Inf, ymax=200, fill="blue", 
           alpha=0.25)  + 
  labs(x="Time", y="Adjusted Value", title="Apple Stock Values from 2008-2011")

```

Forecasts Are Tricky
========================

- Fit another model on the middle part of the data (purple)

```{r, echo=FALSE, results='hide', fig.align='center', fig.width=9}
qplot(time2, AAPL.Adjusted, data = plotdf, geom='line', group=1) +
  geom_smooth(aes_auto(loess.DF2),  data=loess.DF2, stat="identity", 
              se=FALSE, color="purple") +
     coord_cartesian(xlim=c(200,1200), ylim=c(60,700)) +
    geom_smooth(method="lm", linetype=2, color=I("red")) +
  geom_vline(xintercept=idx1, color="red", linetype=3) + 
  geom_vline(xintercept=idx2, color="red", linetype=3) + 
  geom_vline(xintercept=idx3, color="red", linetype=3) + 
  geom_vline(xintercept=idx4, color="red", linetype=3) + 
  geom_vline(xintercept=idx5, color="red", linetype=3) + 
  theme_dpi()+
  theme(axis.text.x=element_blank(), axis.ticks=element_blank(), 
        panel.grid.major.x = element_blank(), 
        panel.grid.minor.x = element_blank()) +
    annotate("rect", xmin=500, xmax=800, ymin=-Inf, ymax=400, 
             fill="purple", alpha=0.25) + 
  labs(x="Time", y="Adjusted Value", title="Apple Stock Values from 2008-2013")


```

Evaluating Model Fit
==================================================

How do we know how well our models fit? A **very brief** model comparison review:

- $\\R^2$ - ratio of explained variation to total variation (generally)
- Nested model tests: 
  * F test and Likelihood ratio tests (restricted and unrestricted model)
- Same sample tests:  
  * AIC, BIC, etc. (different penalties for model parameters)
- These don't give us a sense of how the model will do on **new** data, and they 
are not easy to explain!

Predicting New Data
================================================

- Test both models on the full data!

```{r, echo=FALSE, results='hide', fig.align='center', fig.width=9}
qplot(time2, AAPL.Adjusted, data = plotdf, geom='line', group=1) +
  geom_smooth(aes_auto(loess.DF),  data=loess.DF, stat="identity", se=FALSE) +
  geom_smooth(aes_auto(loess.DF2),  data=loess.DF2, stat="identity", 
              se=FALSE, color="purple") +
     coord_cartesian(xlim=c(0,1400), ylim=c(60,700)) +
  geom_smooth(method="lm", linetype=2, color=I("red")) +
  geom_vline(xintercept=idx1, color="red", linetype=3) + 
  geom_vline(xintercept=idx2, color="red", linetype=3) + 
  geom_vline(xintercept=idx3, color="red", linetype=3) + 
  geom_vline(xintercept=idx4, color="red", linetype=3) + 
  geom_vline(xintercept=idx5, color="red", linetype=3) + 
  theme_dpi()+
  theme(axis.text.x=element_blank(), axis.ticks=element_blank(), 
        panel.grid.major.x = element_blank(), 
        panel.grid.minor.x = element_blank()) +
  annotate("rect", xmin=50, xmax=300, ymin=-Inf, ymax=400, fill="blue", 
           alpha=0.25) +
    annotate("rect", xmin=500, xmax=800, ymin=-Inf, ymax=400, 
             fill="purple", alpha=0.25)
  labs(x="Time", y="Adjusted Value", title="Apple Stock Values from 2008-2013")


```

The Bias - Variance Tradeoff
=============================================

- The purple and blue models are identical except each was "trained" on different 
data, the difference between them is variance
- Both methods have the same bias, but the linear model has a different bias - 
a feature of the flexibility in the model
- Less flexible models will have more bias, but are less variable in response to 
the data they are trained on



Model fit = Fit to signal + fit to noise
============================================

- Training data can lead to model overfit (the blue line)
- Training data can lead to bias in future predictions (the purple line)
- We need both methods of $f$ and methods of evaluating models that 
can insulate against overfit
- This means different measures of model fit to choose among competing models
- Understanding the data generation process helps inform how difficult extrapolation will be
- Time changes everything and the process/logic of updating models is important
- Non-linear behaviors can be right around the corner
- Paradigm shifts occur


Bias, Variance, Training, and Test Data
======================================

<small>Figure from Hastie, Tibshirani and Friedman (2009). Springer-Verlag (Figure 7.1) </small>

<img style="height:auto; width:auto; max-width:600px; max-height:700px;" src="img/ESLFig7.1.png" title="Variance and Bias" alt="ESL7.1" style="display: block; margin: auto;" />



Measuring Fit Differently
=============================

- The more complex the model gets, the more it overfits the training data at the 
cost of the test data!
- Need to estimate the error on the test data set
- Need to choose an error measure: prediction error, mean squared error, etc. 
- Identify the method that is most appropriate: Repeated folds, cross-validation, LOOCV
- All ways to deal with robustly choosing a model that might fit the training 
data less optimally, but it likely to be a better fit on future data

Splitting the Data
============================

- In cases where observations are cheap, 50% of the sample is for training, 25% 
for validation, and 25% for final testing
- When data is not cheap, a number of methods can be used to approximate the 
test set error
- We are familiar with in-sample error estimates such as AIC, BIC, etc.
- K fold cross-validation splits the data into 5 groups, and uses each group 
1 time as a validation set, fitting the model to the other 4 groups
  *  "Overall, ﬁve- or tenfold cross-validation are recommended as a good compromise: see Breiman and Spector (1992)
and Kohavi (1995)." Hastie et al p. 243
- Bootstrap

Confusion Matrix
======================
<table>
  		<tr>
				<td colspan="2" rowspan="2"></td>
				<td colspan="2" style="background-color:#1b85b8; border: 2px solid #aaa">Actual</td>			
			</tr>
			<tr>
				<td>Negative</td>
				<td>Positive</td>
			</tr>
			<tr>
				<td rowspan="2" style="background-color:#ae5a41; border: 2px solid #aaa">Predicted</td>
				<td>Negative</td>
				<td><b>a</b></td>
				<td><b>b</b></td>
			</tr>
			<tr>
				<td>Positive</td>
				<td><b>c</b></td>
				<td><b>d</b></td>
			</tr>
		</table>
    
Some performance metrics we can use:
- Accuracy: $\frac{(a+d)}{(a+b+c+d)}$
- Precision (positive predictive value) = $\frac{a}{(a+b)}$
- Sensitivity (recall) = $\frac{a}{(a+c)}$
- Specificity (negative predictive value) = $\frac{d}{(b+d)}$
- False alarm (1-specificity) = $\frac{b}{(b+d)}$

Confusion Matrix
======================
<table>
      <tr>
				<td colspan="2" rowspan="2"></td>
				<td colspan="2" style="background-color:#1b85b8; border: 2px solid #aaa">Actual</td>			
			</tr>
			<tr>
				<td>Negative</td>
				<td>Positive</td>
			</tr>
			<tr>
				<td rowspan="2" style="background-color:#ae5a41; border: 2px solid #aaa">Predicted</td>
				<td>Negative</td>
				<td  style="background-color:#c3cb71; border: 2px solid #ead61c"><b>a</b></td>
				<td><b>b</b></td>
			</tr>
			<tr>
				<td>Positive</td>
				<td><b>c</b></td>
				<td style="background-color:#c3cb71; border: 2px solid #ead61c"><b>d</b></td>
			</tr>
		</table>
    

Accuracy: $\frac{(a+d)}{(a+b+c+d)}$

Accuracy is a good measure if our classes are fairly balanced and we care about 
overall correctly dividing the data into the groups. 

If one group is much larger than another though, the most accurate model may not 
be the model that most correctly classifies the group we care about. (CITATION)

Confusion Matrix
======================
<table>
    	<tr>
				<td colspan="2" rowspan="2"></td>
				<td colspan="2" style="background-color:#1b85b8; border: 2px solid #aaa">Actual</td>			
			</tr>
			<tr>
				<td>Negative</td>
				<td>Positive</td>
			</tr>
			<tr>
				<td rowspan="2" style="background-color:#ae5a41; border: 2px solid #aaa">Predicted</td>
				<td>Negative</td>
				<td  style="background-color:#c3cb71; border: 2px solid #ead61c"><b>a</b></td>
				<td  style="background-color:#c3cb71; border: 2px solid #ead61c"><b>b</b></td>
			</tr>
			<tr>
				<td>Positive</td>
				<td><b>c</b></td>
				<td><b>d</b></td>
			</tr>
		</table>
    

Precision (negative predictive value) = $\frac{a}{(a+b)}$

- Of all the cases we predict to be negative, what proportion actually are?
- If we are interested in the negative class, then this is a very useful metric 
to understand how good we are at identifying this group. Useful if this class is a rare 
class.

Confusion Matrix
======================
<table>
      <tr>
				<td colspan="2" rowspan="2"></td>
				<td colspan="2" style="background-color:#1b85b8; border: 2px solid #aaa">Actual</td>			
			</tr>
			<tr>
				<td>Negative</td>
				<td>Positive</td>
			</tr>
			<tr>
				<td rowspan="2" style="background-color:#ae5a41; border: 2px solid #aaa">Predicted</td>
				<td>Negative</td>
				<td  style="background-color:#c3cb71; border: 2px solid #ead61c"><b>a</b></td>
				<td ><b>b</b></td>
			</tr>
			<tr>
				<td>Positive</td>
				<td style="background-color:#c3cb71; border: 2px solid #ead61c"><b>c</b></td>
				<td><b>d</b></td>
			</tr>
		</table>
    

Sensitivity (recall) = $\frac{a}{(a+c)}$

- Of all the negative cases, what percentage do we correctly identify (recall)?
- Useful if we are interested in rare-event models where we want to accurately 
identify rare events, and are less worried about how accurate we are with the modal 
or common case. 

Confusion Matrix
======================
<table>
      <tr>
  			<td colspan="2" rowspan="2"></td>
				<td colspan="2" style="background-color:#1b85b8; border: 2px solid #aaa">Actual</td>			
			</tr>
			<tr>
				<td>Negative</td>
				<td>Positive</td>
			</tr>
			<tr>
				<td rowspan="2" style="background-color:#ae5a41; border: 2px solid #aaa">Predicted</td>
				<td>Negative</td>
				<td><b>a</b></td>
				<td style="background-color:#c3cb71; border: 2px solid #ead61c"><b>b</b></td>
			</tr>
			<tr>
				<td>Positive</td>
				<td><b>c</b></td>
				<td style="background-color:#c3cb71; border: 2px solid #ead61c"><b>d</b></td>
			</tr>
		</table>
    

Specificity (positive predictive value) = $\frac{d}{(b+d)}$

False alarm (1-specificity) = $\frac{b}{(b+d)}$

- Of all the positive cases, what proportion actually do we predict correctly?
- If we are interested in one class, this metric is either interesting on its own, 
or as the balancing metric (false alarm) that we seek to hold constant while 
increasing our sensitivity. 

Outline
===================

1. Training and test fit
2. Classification measures
3. Cross-validation
4. Setting thresholds


Model Fit
==================================================

```{r echo=FALSE, fig.width=12, fig.height=8, fig.align='center'}

ews <- read.csv("data/BowersEWSReviewData.csv")

ews$flag <- "Other EWI"
ews$flag[ews$id==1 | ews$id==2] <- "Chicago On-Track"
ews$flag[ews$id > 3 & ews$id <14] <- "Balfanz ABC"
ews$flag[ews$id ==85] <- "Muthèn Math GMM"
ews$flag[ews$id==19] <- "Bowers GPA GMM"
ews$flag <- factor(ews$flag)
ews$flag <- relevel(ews$flag, ref="Other EWI")

mycol <- c("Other EWI" = "gray70", "Chicago On-Track" = "blue", 
           "Balfanz ABC" = "purple", 
           "Muthèn Math GMM" = "orange", 
           "Bowers GPA GMM" = "dark red")

library(grid)

qplot(1-specificity, sensitivity,data=ews, shape=flag, size=I(4), 
     color=flag, geom='point') + scale_shape("EWI Type") +
  scale_color_manual("EWI Type",values=mycol) +
  geom_abline(intercept=0, slope=1, linetype=2) + 
  coord_cartesian(xlim=c(0,1), ylim=c(0,1)) + theme_dpi(base_size=14) +
  labs(x="False Alarm Proportion", y="True Positive Proportion",
       title = "ROC Accuracy of Early Warning Indicators") + 
  theme(legend.position = c(0.8, 0.2), 
        legend.background = element_rect(fill=NULL, color="black")) + 
  annotate(geom="segment", x=0.55, y=0.625, yend = 0.785, xend=0.4, 
           arrow = arrow(length = unit(0.5, "cm"))) + 
  annotate(geom="text", x=.365, y=.81, label="Better Prediction") +
  annotate(geom="segment", x=0.65, y=0.625, yend = 0.5, xend=0.75, 
           arrow = arrow(length = unit(0.5, "cm"))) + 
  annotate(geom="text", x=.75, y=.48, label="Worse Prediction") +
    annotate(geom="text", x=.75, y=.78, angle=37, label="Random Guess")

```

<small>Adapted from Bowers and Sprott 2013</small>


What Changes When a Model is Actually Used?
==================================================


Preparing Data
=================================================

1. Recoding variables
2. Centering and scaling


Missing Data Issues
==================================================


Credits
====================
- Some of the figures in this presentation are taken from "An Introduction to 
Statistical Learning, with applications in R"  (Springer, 2013) with permission 
from the authors: G. James, D. Witten,  T. Hastie and R. Tibshirani
- Watson and Crick photo from: [http://www.thehistoryblog.com/wp-content/uploads/2013/05/Watson-Crick-DNA-model.jpg](http://www.thehistoryblog.com/wp-content/uploads/2013/05/Watson-Crick-DNA-model.jpg)
- CPU power graph: [http://preshing.com/20120208/a-look-back-at-single-threaded-cpu-performance/](http://preshing.com/20120208/a-look-back-at-single-threaded-cpu-performance/)

Further Resources
====================

- The Signal and the Noise: Why So Many Predictions Fail — but Some Don't. Nate Silver. (2012). Penguin.
- The Black Swan: Second Edition: The Impact of the Highly Improbable (2nd ed. 2010). Nassim Taleb.  Random House.
- An Introduction to Statistical Learning (2013). Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. Springer. [Get the book](http://www-bcf.usc.edu/~gareth/ISL/index.html)
- Elements of Statistical Learning (Second Edition, 2011). Trevor Hastie, 
Robert Tibshirani, and Jerome Friedman. Springer [Get the book](http://statweb.stanford.edu/~tibs/ElemStatLearn/)

An Aside on Unsupervised Models
=====================================

```{r clusters, echo=FALSE, results='hide', fig.align='center', fig.height=4.5, fig.width=6, dev.args=list(bg="white")}
x <- rbind(matrix(rnorm(100, sd = 0.3), ncol = 2),
           matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2))
colnames(x) <- c("x", "y")
cl <- kmeans(x, 2)
par(bg = 'white')
plot(x, col = cl$cluster)
points(cl$centers, col = 1:2, pch = 8, cex = 2)
# 
# # sum of squares
# ss <- function(x) sum(scale(x, scale = FALSE)^2)
# 
# ## cluster centers "fitted" to each obs.:
# fitted.x <- fitted(cl);  head(fitted.x)
# resid.x <- x - fitted(cl)
# 
# **Unsupervised** techniques like cluster, principal components, factor, and 
# latent variable analysis can be very useful!
# 
# spec <- read.table("data/benchmarks.txt", header=TRUE, sep="\t")
# spec2 <- read.table("data/summaries.txt", header=TRUE, sep="\t")
```

- These are familiar techniques for dimension reduction like cluster analysis, factor 
analysis, or principal components analysis
- Can be useful for starting an analysis, looking for structure


