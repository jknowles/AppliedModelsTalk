<style>

body, p, td, li, div {
  font-size: 18pt;
  color: white;
}

h1,h2,h3,h4,h5,h6 {
	text-shadow: 0 0 0 #000 !important;
  color: white;
}


.reveal .state-background {
  background: black;
} 

.reveal section p {
  color: white;
}

.reveal section h1 {
  color: white;
}

.reveal section h2 {
  color: white;
}

.reveal section h3 {
  color: white;
}

</style>

Using Statistical Models to Help Policymakers Avoid Mistakes
========================================================
author: Jared Knowles
date: October 23, 2013

Overview
===============

- What mistakes do policymakers make?
- What tools can be used to avoid them?
- How do you make the case to avoid these mistakes?
- Your toolkit needs to be broad to meet policymakers needs

Mistakes We Knew We Were Making
======================================

Three vignettes will serve to ground our discussion on common mistakes in 
the policy world using data. No policymaker is immune to making any of these 
mistakes and even trained analysts can, do, and will make them.

1. The case of the simple mean
2. Why rates are always leading us astray?
3. Applying models improperly


Problems with the mean
========================

1. The mean is uninformative because it masks conditional variation
2. The mean invites overinterpretation too often
3. The mean too often substitutes important facts about the rest of the distribution

Expecting and presenting only the mean masks too much heterogeneity and too much 
important variability to invite systemic or productive analysis. 

The Mean is Uninformative
============================

Gap

- Black-white gap is 1 standard deviation
- Economic disadvantage gap is 1 standard deviation
- SwD gap is 1.2 standard deviations

***

Alternative hypothesis

- Black students are more likely to be poor
- Poor students are more likely to be black
- Disabled students are more likely to be black or poor

The Mean is Misinterpreted
=============================

Gap

- Black-white gap is 1 standard deviation
- Economic disadvantage gap is 1 standard deviation
- SwD gap is 1.2 standard deviations

***

Alternative: 

- All black students underperform white students
- Economically disadvantaged students do not score highly on tests
- Having any disability drastically reduces your student performance


The Mean Masks too Much
===================================

Gap

- Black-white gap is 1 standard deviation
- Economic disadvantage gap is 1 standard deviation
- SwD gap is 1.2 standard deviations

***

Hidden facts: 

- In some schools a white-black gap exists
- Some economically disadvantaged students are indistinguishable from their peers
- Some disability types outperform students without disabilities

The Kicker
===========================

Students who are black, economically disadvantaged, and disabled have much worse 
student outcomes than would ever be considered from the statistics above. 

Students who are white, economically disadvantaged, and disabled have much better 
outcomes than their black peers. 

These facts cannot be uncovered by simple means. 

An Aside on Skew
=============================

```{r graph1, echo=FALSE, results='hide', fig.align='center', fig.width=11}
library(eeptools)
skews <- rgamma(10000, 3, 3)
skews <- c(skews, rep(4, 100), rep(5, 100), rep(6, 50), rep(7, 1000))
m1 <- mean(skews)
m2 <- median(skews)


qplot(skews) + theme_dpi() + geom_vline(xintercept=m1, size=I(1.1), color="red", 
                                        linetype=2) + 
  geom_vline(xintercept=m2, size=I(1.1), color="blue", linetype=1) +
  labs(x="Income in $1,000s", y="Count", title="Skewed Distributions")

```


Tools to Address the Mean Problem
===========================================

- Data visualizations
- Conditional probability models
- Discussions

```{r graph2, echo=FALSE, results='hide', fig.align='center', fig.width=11}
black <- rnorm(1000, -1, 1)
white <- rnorm(1000, 0, 1)

plotdf <- data.frame(scores = c(black, white), race = c(rep("b", 1000), rep("w", 1000)))

qplot(scores, data=plotdf, geom="density", fill=race, alpha=I(0.2)) + theme_dpi() + 
  labs(x="Score", y="Density", title="Simulated Density Curves by Race")

```

Plot The Effects
=========================
```{r effplot1, echo=FALSE, results='hide', fig.align='center', fig.width=11}
library(effects)

mod.cowles <- glm(volunteer ~ sex + neuroticism + extraversion + neuroticism*extraversion, 
    data=Cowles, family=binomial)
eff.cowles <- allEffects(mod.cowles, xlevels=list(neuroticism=0:24, 
    extraversion=seq(0, 24, 6)), given.values=c(sexmale=0.5))
# #eff.cowles
# 
#   # the following are equivalent:
# 
# eff.ne <- effect("neuroticism*extraversion", mod.cowles)
# Eff.ne <- Effect(c("neuroticism", "extraversion"), mod.cowles)
# all.equal(eff.ne$fit, Eff.ne$fit)

plot(effect("neuroticism*extraversion", mod.cowles))

```

Plot The Effects: Categorical
==============================
```{r effplot2, echo=FALSE, results='hide', fig.align='center', fig.width=11}
plot(eff.cowles, 'sex', ylab="Prob(Volunteer)")

```

Problems with the rate
========================

1. The rate is not robust in small groups
2. The rate masks uncertainty
3. The rate does not provide a proper sense of inertia

Rates are often turned to when wishing to explain the differences between areas 
with different group sizes. This can be misused and cause bad policy responses. 


Graduation rates
=========================

Exercise: Describe a school district with the lowest graduation rate in 
your favorite state. 


Graduation rates
=========================

What features did you use to describe it? 

1. High poverty
2. High minority
3. Large urban core?

In Wisconsin the lowest graduation rate in the state belongs to a district with 
under 250 students in a graduating class. 

Only **103** out of **228** students in Grantsburg graduated in 2011-12. 

Two Comments and a Question
===============================

1. If Grantsburg graduated 50 more students in a year, it would move past several 
other districts in the state. 

2. If MPS graduated 500 less students in a year, it still would not fall below 
Grantsburg. 

Is it fair to rank Grantsburg below Milwaukee? It probably depends!

Fragility of Rates
======================

Consider Prentice. 

```{r pregraph, echo=FALSE, results='hide', fig.align='center', fig.width=11}
library(scales)
plotdf <- data.frame(year = c(2010, 2011, 2012), 
                     cohort = c(40, 40, 32), 
                     grads = c(36, 33, 31))


qplot(year, grads/cohort, data=plotdf, geom="bar", stat="identity") + theme_dpi() +
  scale_y_continuous("Graduation Rate", label=percent) + 
  labs(title = "Prentice 4 Year Graduation Rate") + 
  geom_hline(yintercept=0.89, color=I("red"), size=I(1.1))

```

The Problem
====================

1. Is Prentice above or below average?
2. What will Prentice's rate be next year?
3. How easy is it to move the rate?

And all of these problems are if we ignore the problems about unconditional 
probabilities we have already discussed!

Applicability
==========================

This is why ranking systems are suspect rating period to rating period. Simply 
reporting the rate provides no underlying sense of the variability in the measure and 
without also reporting the *n* count, there is no way to interpret the findings. 

If you work in policy you will be asked to report means and rates all the time. 

You will be asked to construct rankings and indexes all the time. 

Tread carefully!

Solutions
=============

1. Present rates with robustness by simulating change in rate with change in 
unit level 
2. Conduct ecological inference on group data to infer probabilities for 
individual units with uncertainty
3. Use Bayesian methods to pool across groups and appropriately estimate uncertainty for small groups, and certainty for large groups

Solutions - Quick and Easy
==============================

For repeated measures we can report the rate with "fragility" by showing the 
range if 3 or 4 more students graduated or did not graduate.

```{r pregraph2, echo=FALSE, results='hide', fig.align='center', fig.width=11}
library(scales)
plotdf <- data.frame(year = c(2010, 2011, 2012), 
                     cohort = c(40, 40, 32), 
                     grads = c(36, 33, 31))


plotdf$rate <- plotdf$grads/plotdf$cohort
plotdf$rate_min <- (plotdf$grads - (.1*plotdf$grads))/plotdf$cohort
plotdf$rate_max <- (plotdf$grads + (.1*plotdf$grads))/plotdf$cohort
plotdf$rate_max <- ifelse(plotdf$rate_max < 1, plotdf$rate_max, 1)

ggplot(aes(x=factor(year), y=rate, ymin=rate_min, ymax=rate_max),data=plotdf) + 
  geom_pointrange(size=I(1.1)) + geom_errorbar(width=0.3, size=I(1.1)) + theme_dpi() + 
  scale_y_continuous("Graduation Rate", label=percent, limits=c(0,1)) + 
  labs(title = "Prentice 4 Year Graduation Rate ROBUST", x="Year") + 
  geom_hline(yintercept=0.89, color=I("red"), size=I(1.1), linetype=2)

```




```{r ecosetup, echo=FALSE, results='hide'}
library(eiPack)
data(senc)
# myEI <- ei.reg(cbind(dem, rep, non) ~ cbind(black, white, natam), data=senc)
# myEIB <- ei.reg.bayes(cbind(dem, rep, non) ~ cbind(black, white, natam), data=senc)

```

Solutions - Interesting
=============================

Without individual level data, we can use something called *ecological 
inference* to try to understand the probability of individual behavior. 

```{r eco1, echo=TRUE, results='markup'}
print(head(senc[, 2:9]))
```

Solutions - Ecological Inference
=================================

```{r eco2, echo=TRUE, results='markup'}
myEI.MD <- ei.MD.bayes(cbind(dem, rep, non) ~ cbind(black, white, natam), 
                       data=senc)
print(myEI.MD)
```

Solutions - Graphical
==========================

```{r ecoplot, echo=FALSE, fig.align='center', fig.width=11}
cover.plot(myEI.MD, "white", "dem")
```

Solutions - Complex and More Useful
=====================================

Statistical models that correct for small group bias and draw information 
from other sources.

[The Radon Project](http://www.stat.columbia.edu/~radon/index.html)

- An online calculator to assess your localized risk for radon based on three factors

1. The estimated countywide probability of severe radon exposure in your area
2. The updated estimate based on test results of nearby neighbors (self-reported)
3. Updated estimate based on 1, 2, and your own home test. 

All weighted by your tolerance for risk and other risk factors that may be larger 
than radon exposure. 



What is a model?
===============================

- An abstraction from reality
- What features does it have?
- What purpose does it serve?

<img src="img/pictureaf1art1.jpg" title="747 Model" alt="747Model" style="display: block; margin: auto;" />

Is this  a model?
===============================

<img src="img/dalek_blueprint.jpg" title="Dalek Model" alt="747Model" style="display: block; margin: auto;" />


What is a statistical model?
===============================

- "All models are wrong, some models are useful"
- Being wrong is a **feature of a statistical model**, the goal is to explain 
as much data as possible with as few variables as possible
- Statistical models are mathematical summaries of correlations and probabilities
- The most common form is linear regression models


Do machines really learn?
========================================================

Applied modeling goes by many names: statistical learning, machine learning, 
predictive analytics, and data mining. 

The key differences between applied modeling and statistical inference are:

- Emphasis on predictive validity
- De-emphasis on parameter values
- Test and training data
- Measures of fit


Goals for This Talk
========================================================

1. Introduce the world of statistical models outside of 
linear models
2. Demonstrate the specific techniques around building predictive models
3. Discuss the tradeoffs in applied vs. research models
4. Show some R code!


Applied Models and Inference
========================================================

Applied modeling and inferential statistics share many of the same concepts:

- Regression estimation
- Concerns about representativeness of data and samples
- Fear of outliers
- Robustness and sensitivity

```{r, echo=FALSE, fig.align='center', fig.height=3.5, fig.width=8}
library(eeptools)
qplot(speed, dist, data=cars) + theme_dpi() + geom_smooth(method='lm', se=FALSE) +
  geom_smooth(method="loess", color=I("purple"), se=FALSE)+
  annotate("segment", x=24, xend=24, y=120, yend=77, color="red", lty = 3, size=1.1)

```


Supervised vs. Unsupervised Learning
===========================================================

A key distinction in statistical learning is that between **supervised** and 
**unsupervised** techniques. 

- **supervised** - relationship between inputs and outputs is being explored
- **unsupervised** - the relationship among inputs is being explored, no output

We will focus on **supervised** learning for the most part in this talk. 

```{r clusters, echo=FALSE, results='hide', fig.align='center', fig.height=3, fig.width=4}
x <- rbind(matrix(rnorm(100, sd = 0.3), ncol = 2),
           matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2))
colnames(x) <- c("x", "y")
cl <- kmeans(x, 2)
plot(x, col = cl$cluster)
points(cl$centers, col = 1:2, pch = 8, cex = 2)
# 
# # sum of squares
# ss <- function(x) sum(scale(x, scale = FALSE)^2)
# 
# ## cluster centers "fitted" to each obs.:
# fitted.x <- fitted(cl);  head(fitted.x)
# resid.x <- x - fitted(cl)
# 
# **Unsupervised** techniques like cluster, principal components, factor, and 
# latent variable analysis can be very useful!
```


Statistical Modeling
=======================================================

It is useful to remember that in statistical modeling, in the **supervised** case, we are looking at the following relationship:

$$ \hat{Y} = \hat{f}(X) $$

In this case $\hat{f}$ represents our estimate of the function that links $X$ and 
$Y$. In traditional linear modeling, $\hat{f}$ takes the form:

$$ \hat{Y} = \alpha + \beta(X) + \epsilon $$

However, there exist limitless alternative $\hat{f}$ which we can explore. Applied modeling techniques help us expand the $\hat{f}$ space we search within.

How do we choose f?
===================================================

Choosing $f$ is about tradeoffs, the most obvious is between flexibility and 
interpretability.

```{r, echo=FALSE, fig.align='center', fig.width=11.5}
library(eeptools)
x    <- c(1, 2, 3, 4, 5, 6, 7, 8)
y    <- c(14, 12, 10, 8, 6, 4, 2, 0)

jit <- function(x) x + runif(1, min=-.5, max=.5)
x <- sapply(x, jit)
y <- sapply(y, jit)

labs <- c("Lasso", "Subset Selection", "Least Squares", 
          "Generalized Additive Models", "KNN", "Trees", "Bagging, Boosting", 
          "Support Vector Machines")

qplot(x, y, geom='text', label=labs) + theme_dpi() + 
  scale_x_continuous("Flexibility", limits=c(min(x) - 0.5, max(x) + 0.5)) +
  scale_y_continuous("Interpretability", limits=c(min(y) - 0.5, max(y) + 0.5)) +
  labs(title="Functional Forms and Tradeoffs") + 
  theme(axis.text=element_blank(), axis.ticks=element_blank())
```


Why the Difference?
========================================================

Applied Models:

- Provide information to users about what to expect given certain data
- Goals for the model are defined by the needs of the users

***

Inferential Models: 

- Seek to learn about the relationships in the data at hand
- Focused on understanding patterns in the current data


Some Vocabulary
========================================================

- Training data
- Test data
- Bias (error)
- Variance (error)


***

- Data the model is fit to
- Data the model is applied to, but not fit to, to evaluate model fit
- Refers to the amount of error due to simplifying a complex process
- The amount the $f$ would change if fit to a different training set of data


The Challenge
=================================

- When using a statistical model to make predictions we have to think clearly 
about the data we use to build the model, and the data we will be making 
predictions about
- We may build a model with high **internal validity** for the data at hand, 
but that data may not be representative of the data the model will apply to
- We call this the **training error** and the **test error**
- In inferential statistics we often seek to reduce **training error** and not 
concern ourselves with **test error**


A Trivial Example
===============================

Consider the following training data:

```{r, echo=FALSE, results='hide', fig.align='center', fig.width=9}
library(MASS)
library(Matrix)
set.seed(8742)
N <- 1000
J <- 4 ### Number of predictors (including intercept)
G <- 5 ### Number of Multilevel Groups
X <- matrix(rnorm(N*J,0,2),N,J)
X[,1] <- 1
Sigma <- matrix(runif(J*J,-1,1),J,J)
Sigma <- nearPD(Sigma)$mat
#diag(Sigma) <- runif(J,1,5)
gamma <- runif(J,-1.2,1.2)
beta <- matrix(NA,G,J)
for (g in 1:G) {beta[g,] <- mvrnorm(1, gamma, Sigma)}
m <- sample(1:G, N, replace=TRUE) ### Multilevel group indicator
y <- rowSums(beta[m,] * X) + rnorm(N,0,.2)

plotdf <- cbind(X, m, y)
plotdf <- as.data.frame(plotdf)

mod1 <- lm(y ~ V2 + V3 + V4 + factor(m), data=plotdf[plotdf$m>3,])
mod2 <- lm(y ~ V2 + V3 + V4 + factor(m), data=plotdf[plotdf$m<=3,])

mod1.plot <- fortify(mod1)
mod2.plot <- fortify(mod2)

plotdf$sample <- NA
plotdf$sample[plotdf$m>3] <- "A"
plotdf$sample[plotdf$m<=3] <- "B"


qplot(V2, y, data=plotdf[plotdf$m>3,]) + 
  geom_smooth(data=mod1.plot, aes(x=V2, y=.fitted), se=FALSE, size=I(1.1),
              method=lm) +
  coord_cartesian(xlim=c(-7,7), ylim=c(-12,13)) +
  theme_dpi()

```

Consider the Test Data
=========================

How does our model fit the test data? 

```{r, echo=FALSE, results='hide', fig.align='center', fig.width=9}
qplot(V2, y, data=plotdf[plotdf$m<=3,], alpha=I(0.6)) + 
  geom_smooth(data=mod1.plot, aes(x=V2, y=.fitted), se=FALSE, size=I(1.6),
              method=lm) +
    coord_cartesian(xlim=c(-7,7), ylim=c(-12,13)) +
  geom_smooth(linetype=2, se=FALSE, size=I(1.6), color=I("purple"))+
  theme_dpi()
```

Consider the Pooled Data
==========================

```{r, echo=FALSE, results='hide', fig.align='center', fig.width=9}
qplot(V2, y, data=plotdf, color=sample) + geom_smooth(method=lm, aes(group=1))+
  geom_smooth(data=mod1.plot, aes(x=V2, y=.fitted, color=NULL), 
              se=FALSE, size=I(1.1),method=lm) +  
     geom_smooth(data=mod2.plot, aes(x=V2, y=.fitted, color=NULL), 
              se=FALSE, size=I(1.1),method=lm) + 
      coord_cartesian(xlim=c(-7,7), ylim=c(-12,13)) +
  theme_dpi()

```

What do we learn?
=============================

- The data was generated from the same function but there was a trend across 
groups
- Predicting from this training model leads to **bias** in our predictions
- The relationship among the groups iteratively shifted, so data earlier in 
the process understated the effect coming later
- These out of sample differences are what make forecasting tricky
- Traditional methods rely on sampling to do this
- How do we protect ourselves in the case when we can't sample the data we want 
to predict because it hasn't been generated yet? 


When Could this Matter: Stocks?
=================================

```{r, echo=FALSE, results='hide', fig.align='center', fig.width=9}
library('quantmod')
library('lubridate')

getSymbols("AAPL")
AAPL$year <- year(index(AAPL))
AAPL$yearL <- lag(AAPL$year)
AAPL$yearSTART <- 0
AAPL$yearSTART[AAPL$year - AAPL$yearL > 0] <- 1
AAPL$yearL <- NULL
AAPL$lag <- lag(AAPL$AAPL.Adjusted)
AAPL$lag2 <- lag(AAPL$AAPL.Adjusted, 2)
AAPL$lag3 <- lag(AAPL$AAPL.Adjusted, 3)
AAPL$lag30 <- lag(AAPL$AAPL.Adjusted, 30)
AAPL$lag90 <- lag(AAPL$AAPL.Adjusted, 90)

plotdf <- as.data.frame(AAPL)
plotdf$time <- row.names(plotdf)
plotdf <- plotdf[300:1700,]
plotdf$group <- NA
plotdf$group[50:300] <- 1
plotdf$group[300:500] <- 2

plotdf$time2 <- as.factor(plotdf$time)
plotdf$time2 <- as.numeric(plotdf$time2)

pred.con <- loess.control(surface="direct")

model1 <- loess(AAPL.Adjusted ~ time2 + lag + lag30, 
                data=plotdf[plotdf$group==1,], 
                control=pred.con, parametric=c("lag"))

xrange <- range(plotdf$time2)
xseq <- seq(from=xrange[1], to=xrange[2], length=nrow(plotdf))
pred <- predict(model1, newdata = data.frame(time2 = xseq, lag=plotdf$lag, 
                                             lag30 = plotdf$lag30,
                                             lag90 = plotdf$lag90), se=TRUE)
y = pred$fit
ci <- pred$se.fit * qt(0.95 / 2 + .5, pred$df)
ymin = y - ci
ymax = y + ci
loess.DF <- data.frame(x = xseq, y) #, ymin, ymax, se = pred$se.fit)

model2 <- loess(AAPL.Adjusted ~ time2 + lag + lag30, 
                data=plotdf[500:800,], 
                control=pred.con,  parametric=c("lag"))

xrange <- range(plotdf$time2)
xseq <- seq(from=xrange[1], to=xrange[2], length=nrow(plotdf))
pred <- predict(model2, newdata = data.frame(time2 = xseq, lag=plotdf$lag, 
                                             lag30 = plotdf$lag30,
                                             lag90 = plotdf$lag90), se=TRUE)
y = pred$fit
ci <- pred$se.fit * qt(0.95 / 2 + .5, pred$df)
ymin = y - ci
ymax = y + ci
loess.DF2 <- data.frame(x = xseq, y)


idx1 <- plotdf$time2[plotdf$yearSTART==1][5]
idx2 <- plotdf$time2[plotdf$yearSTART==1][4]
idx3 <- plotdf$time2[plotdf$yearSTART==1][3]


qplot(time, AAPL.Adjusted, data = plotdf, geom='line', group=1) +
     coord_cartesian(xlim=c(0,1400), ylim=c(60,700)) +
  geom_vline(xintercept=idx1, color="red", linetype=3) + 
  geom_vline(xintercept=idx2, color="red", linetype=3) + 
  geom_vline(xintercept=idx3, color="red", linetype=3) + 
  theme_dpi()+
  theme(axis.text.x=element_blank(), axis.ticks=element_blank(), 
        panel.grid.major.x = element_blank(), 
        panel.grid.minor.x = element_blank()) 

```


Forecasting Apple Stock Could be Useful
===========================================

```{r, echo=FALSE, results='hide', fig.align='center', fig.width=9}

qplot(time, AAPL.Adjusted, data = plotdf, geom='line', group=1) +
  geom_smooth(aes_auto(loess.DF),  data=loess.DF, stat="identity", se=FALSE) +
     coord_cartesian(xlim=c(0,800), ylim=c(60,700)) +
  geom_vline(xintercept=idx1, color="red", linetype=3) + 
  geom_vline(xintercept=idx2, color="red", linetype=3) + 
  geom_vline(xintercept=idx3, color="red", linetype=3) + 
  theme_dpi()+
  theme(axis.text.x=element_blank(), axis.ticks=element_blank(), 
        panel.grid.major.x = element_blank(), 
        panel.grid.minor.x = element_blank()) 

```

Forecasts Are Tricky
========================

```{r, echo=FALSE, results='hide', fig.align='center', fig.width=9}
qplot(time, AAPL.Adjusted, data = plotdf, geom='line', group=1) +
  geom_smooth(aes_auto(loess.DF2),  data=loess.DF2, stat="identity", 
              se=FALSE, color="purple") +
     coord_cartesian(xlim=c(500,1400), ylim=c(60,700)) +
    geom_smooth(method="lm", linetype=2, color=I("red")) +
  geom_vline(xintercept=idx1, color="red", linetype=3) + 
  geom_vline(xintercept=idx2, color="red", linetype=3) + 
  geom_vline(xintercept=idx3, color="red", linetype=3) + 
  theme_dpi()+
  theme(axis.text.x=element_blank(), axis.ticks=element_blank(), 
        panel.grid.major.x = element_blank(), 
        panel.grid.minor.x = element_blank()) 

```

The Further We Get From The Training Data...
================================================


```{r, echo=FALSE, results='hide', fig.align='center', fig.width=9}
qplot(time, AAPL.Adjusted, data = plotdf, geom='line', group=1) +
  geom_smooth(aes_auto(loess.DF),  data=loess.DF, stat="identity", se=FALSE) +
  geom_smooth(aes_auto(loess.DF2),  data=loess.DF2, stat="identity", 
              se=FALSE, color="purple") +
     coord_cartesian(xlim=c(0,1400), ylim=c(60,700)) +
  geom_smooth(method="lm", linetype=2, color=I("red")) +
  geom_vline(xintercept=idx1, color="red", linetype=3) + 
  geom_vline(xintercept=idx2, color="red", linetype=3) + 
  geom_vline(xintercept=idx3, color="red", linetype=3) + 
  theme_dpi()+
  theme(axis.text.x=element_blank(), axis.ticks=element_blank(), 
        panel.grid.major.x = element_blank(), 
        panel.grid.minor.x = element_blank()) 

```


Overfit
=====================

- Training data can lead to model overfit
- We need both methods of **f** and methods of evaluating models that 
can insulate against overfit
- This means different measures of model fit
- Extrapolation gets harder
- Time changes everything
- Non-linear behaviors
- Paradigm shifts


Measuring Fit Differently
=============================

- Classification measures
- Mean Squared Error for the test data
- Folding, cross validation, and other methods of measuring error


Takeaways
================

- Linear modeling and regression is just one of many choices of modeling data
- Model fit within the data may not provide reliable or useful results outside 
of the data
- Different problems call for different statistical modeling techniques
- These techniques include different functional forms, different measures of 
model fit, and different ways of classifying successful models
- Models are tools to understand complex relationships


Keep an open mind
===============================

- K Nearest Neighbors

<img src="img/Map1NN.png" title="KNN Algorithm" alt="KNN" style="display: block; margin: auto;" />


Further Resources
====================

- Signal and Noise. Nate Silver.
- Black Swan. Nassim Taleb.
- An Introduction to Statistical Learning (2013). Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. Springer. 
- Elements of Statistical Learning (Second Edition, 2011). Trevor Hastie, 
Robert Tibshirani, and Jerome Friedman. Springer
